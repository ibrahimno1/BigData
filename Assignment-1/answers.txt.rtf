{\rtf1\ansi\ansicpg1252\cocoartf2761
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid1\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid1}
{\list\listtemplateid2\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat2\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid101\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid2}
{\list\listtemplateid3\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat3\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid201\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid3}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}{\listoverride\listid2\listoverridecount0\ls2}{\listoverride\listid3\listoverridecount0\ls3}}
\margl1440\margr1440\vieww34360\viewh21600\viewkind0
\deftab720
\pard\pardeftab720\li960\fi-960\sa213\partightenfactor0

\f0\fs32 \cf0 \expnd0\expndtw0\kerning0
CMPT 732 \'96 Assignment 1 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0 Ibrahim Ali\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa213\partightenfactor0
\ls1\ilvl0\cf0 \kerning1\expnd0\expndtw0 {\listtext	1	}\expnd0\expndtw0\kerning0
How did the output change when you submitted with\'a0-D mapreduce.job.reduces=3? Why would this be necessary if your job produced large output sets?\
\pard\pardeftab720\li960\fi-960\sa213\partightenfactor0
\cf0 The output went from having one single output file, with mapreduce.job.reduces=3 had three output files, part-r-00000, part-r-00001, and part-r-00002\
\pard\pardeftab720\sa213\partightenfactor0
\cf0 Main benefit of using multiple reducers is performance improvement for large data sets by distributing the workload across reducers. This is accomplished by speeding up the job and distributing the load across different nodes in the cluster. Additionally, each reducer gets a partition of the data, which prevents memory overload on a single node and ensures that the workload is evenly distributed.\
\pard\pardeftab720\li1477\fi-1478\sa213\partightenfactor0
\cf0 \'a0\
\pard\pardeftab720\li960\fi-960\sa213\partightenfactor0
\cf0 \'a0\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa213\partightenfactor0
\ls2\ilvl0\cf0 \kerning1\expnd0\expndtw0 {\listtext	2	}\expnd0\expndtw0\kerning0
How was the\'a0-D mapreduce.job.reduces=0\'a0output different?\
\pard\pardeftab720\li960\fi-960\sa213\partightenfactor0
\cf0 The output is directly written by the mapper, with each mapper writing a speratefile. When running mapreduce.job.reduces=0\'a0, the output had 19 files, one for each input in directory wordcount-1.\
\'a0\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa213\partightenfactor0
\ls3\ilvl0\cf0 \kerning1\expnd0\expndtw0 {\listtext	3	}\expnd0\expndtw0\kerning0
Was there any noticeable difference in the running time of your\'a0RedditAverage\'a0with and without the combiner optimization?\
\pard\pardeftab720\sa213\partightenfactor0
\cf0 No there was not any noticeable difference in the running time, this is due to the sample data being small. However, with bigdata there will be a noticeable difference. Without combiners, it can take a longer time due to the data being shuffled across the cluster, in comparison to with having a combiner, \'a0the data operates locally, removing the need for data to talk on the network.\
}